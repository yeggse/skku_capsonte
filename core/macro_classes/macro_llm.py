import json
import os
from dataclasses import dataclass

import numpy as np
import pandas as pd
import requests
# import shap  # Not used - removed to avoid matplotlib compatibility issues
from openai import OpenAI
from dotenv import load_dotenv

from typing import Dict, List, Optional, Literal, Tuple, Any
from collections import defaultdict
import torch
from config.agents_set import dir_info

load_dotenv()

# -----------------------------
# ë°ì´í„° êµ¬ì¡° ì •ì˜
# -----------------------------
@dataclass
class Target:
    """ì˜ˆì¸¡ ëª©í‘œê°’ + ë¶ˆí™•ì‹¤ì„± ì •ë³´ í¬í•¨
    - next_close: ë‹¤ìŒ ê±°ë˜ì¼ ì¢…ê°€ ì˜ˆì¸¡ì¹˜
    - uncertainty: Monte Carlo Dropout ê¸°ë°˜ ì˜ˆì¸¡ í‘œì¤€í¸ì°¨(Ïƒ)
    - confidence: ëª¨ë¸ ì‹ ë¢°ë„ Î² (ì •ê·œí™”ëœ ì‹ ë¢°ë„; ì„ íƒì )
    """
    next_close: float
    uncertainty: Optional[float] = None
    confidence: Optional[float] = None
    feature_cols: Optional[List[str]] = None
    importances: Optional[List[float]] = None

@dataclass
class Opinion:
    agent_id: str
    target: Target
    reason: str

@dataclass
class Rebuttal:
    from_agent_id: str
    to_agent_id: str
    stance: Literal["REBUT", "SUPPORT"]
    message: str

@dataclass
class RoundLog:
    round_no: int
    opinions: List[Opinion]
    rebuttals: List[Rebuttal]
    summary: Dict[str, Target]

@dataclass
class StockData:
    agent_id: str = ""
    ticker: str = ""
    X: Optional[np.ndarray] = None
    y: Optional[np.ndarray] = None
    feature_cols: Optional[List[str]] = None
    last_price: Optional[float] = None
    technical: Optional[Dict] = None

    def __post_init__(self):
        if self.last_price is None:
            self.last_price = 100.0


# ==============================================================
# 1ï¸âƒ£ LLM ê¸°ë°˜ ì„¤ëª… ëª¨ë“ˆ (í™•ì¥í˜•)
# ==============================================================
class LLMExplainer:
    OPENAI_URL = "https://api.openai.com/v1/chat/completions"

    def __init__(self, model_name="gpt-4o-mini",
                 model: Optional[str] = None,
                 preferred_models: Optional[List[str]] = None,
                 temperature: float = 0.2,
                 verbose: bool = False,
                 need_training: bool = True,
                 ):

        self.api_key = os.getenv("CAPSTONE_OPENAI_API")
        if not self.api_key:
            raise RuntimeError("í™˜ê²½ë³€ìˆ˜ CAPSTONE_OPENAI_APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self.client = OpenAI(api_key=self.api_key)
        self.model = model_name

        self.agent_id = 'MacroAgent'
        self.temperature = temperature # Temperature ì„¤ì •
        self.verbose = verbose            # ë””ë²„ê¹… ëª¨ë“œ
        self.need_training = need_training # ëª¨ë¸ í•™ìŠµ í•„ìš” ì—¬ë¶€
        # ëª¨ë¸ í´ë°± ìš°ì„ ìˆœìœ„
        self.preferred_models = preferred_models or ["gpt-5-mini", "gpt-4.1-mini"]
        if model:
            self.preferred_models = [model] + [
                m for m in self.preferred_models if m != model
            ]

        # ê³µí†µ í—¤ë”
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        # ìƒíƒœê°’
        self.stockdata: Optional[StockData] = None
        self.opinions: List[Opinion] = []
        self.rebuttals: Dict[int, List[Rebuttal]] = defaultdict(list)

        # JSON Schema
        self.schema_obj_opinion = {
            "type": "object",
            "properties": {
                "next_close": {"type": "number"},
                "reason": {"type": "string"},
            },
            "required": ["next_close", "reason"],
            "additionalProperties": False,
        }
        self.schema_obj_rebuttal = {
            "type": "object",
            "properties": {
                "stance": {"type": "string", "enum": ["REBUT", "SUPPORT"]},
                "message": {"type": "string"},
            },
            "required": ["stance", "message"],
            "additionalProperties": False,
        }

    def generate_explanation(
            self,
            feature_summary,
            predictions,
            importance_summary,
            temporal_summary=None,
            consistency_summary=None,
            sensitivity_summary=None,
            stability_summary=None,
            stock_data=None,
            target=None,
    ):
        """
        Gradient Ã— Input / Integrated Gradients ê¸°ë°˜ feature importance ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ
        LLMì´ ë…¼ë¦¬ì  ê¸ˆìœµ ë¶„ì„ì„ ìƒì„±í•˜ë„ë¡ í•˜ëŠ” ë²„ì „
        """

        def _summarize(obj, max_len=1500):
            text = str(obj)
            if len(text) > max_len:
                text = text[:max_len] + "\n...(truncated)"
            return text

        # âœ… ì•ˆì „í•œ ë¬¸ìì—´ ë³€í™˜
        importance_summary = _summarize(importance_summary)
        temporal_summary = _summarize(temporal_summary)
        consistency_summary = _summarize(consistency_summary)
        sensitivity_summary = _summarize(sensitivity_summary)
        stability_summary = _summarize(stability_summary)

        # 1ï¸âƒ£ system ë©”ì‹œì§€
        sys_text = (
            "ë„ˆëŠ” ê¸ˆìœµ ì‹œì¥ì„ ë¶„ì„í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ì• ë„ë¦¬ìŠ¤íŠ¸ì´ë‹¤. "
            "Gradient Ã— Input ë° Integrated Gradients ê¸°ë°˜ì˜ LSTM ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í•´ì„í•´ì•¼ í•œë‹¤. "
            "ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’, ë³€ìˆ˜ ì¤‘ìš”ë„, ì‹œê°„ì  ë³€í™”, ì¼ê´€ì„±, ë¯¼ê°ë„, ì•ˆì •ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ "
            "ê²½ì œì  ì˜ë¯¸ë¥¼ ë„ì¶œí•˜ë¼."
        )

        # 2ï¸âƒ£ user ë©”ì‹œì§€ (Gradient ê¸°ë°˜ ë¶„ì„ ì¤‘ì‹¬)
        user_text = f"""
        ### 1. ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼
        {predictions}
    
        ### 2. ì£¼ìš” ë³€ìˆ˜ ì¤‘ìš”ë„ ìš”ì•½ (feature_summary)
        {feature_summary}
    
        ### 3. ì „ì²´ ë³€ìˆ˜ ì¤‘ìš”ë„ ë§µ (importance_dict)
        {importance_summary}
    
        ### 4. ìƒìœ„ ë³€ìˆ˜ ë° ì‹œì ë³„ ì˜í–¥ ë³€í™” (temporal_summary)
        {temporal_summary}
    
        ### 5. IG / GÃ—I ê°„ ì¼ê´€ì„± ë¶„ì„ (consistency_summary)
        {consistency_summary}
    
        ### 6. ì…ë ¥ ë³€í™” ë¯¼ê°ë„ ë¶„ì„ (sensitivity_summary)
        {sensitivity_summary}
    
        ### 7. ë³€ìˆ˜ ì¤‘ìš”ë„ ì•ˆì •ì„± ë¶„ì„ (stability_summary)
        {stability_summary}
    
        ---  
        ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ í•­ëª©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì²´ê³„ì ì´ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
        
        (1) **Feature Trend (Temporal) ë¶„ì„:**
            - ì–´ë–¤ ë³€ìˆ˜ë“¤ì˜ ì˜í–¥ë ¥ì´ ìµœê·¼ ì‹œì ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ì»¤ì¡ŒìŠµë‹ˆê¹Œ?
            - ë°˜ëŒ€ë¡œ ì˜í–¥ë ¥ì´ ì•½í™”ëœ ë³€ìˆ˜ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?
            - ì´ëŸ¬í•œ ë³€í™”ê°€ ë‚˜íƒ€ë‚œ ê±°ì‹œì Â·ì‚°ì—…ì  ìš”ì¸ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?
            - ì‹œê°„ íë¦„ì— ë”°ë¥¸ ë³€ìˆ˜ ì˜í–¥ ë³€í™”ê°€ ì˜ˆì¸¡ ë°©í–¥ì— ì–´ë–¤ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ì§€ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.
        
        (2) **Model Consistency ë¶„ì„:**
            - Integrated Gradientsì™€ Gradient Ã— Input ê²°ê³¼ê°€ ì¼ì¹˜í•˜ëŠ” ì£¼ìš” featureì™€ ë¶ˆì¼ì¹˜í•˜ëŠ” featureë¥¼ êµ¬ë¶„í•˜ì‹­ì‹œì˜¤.
            - ë¶ˆì¼ì¹˜ê°€ ë†’ì€ featureëŠ” ì–´ë–¤ ì‹œì¥ ë¶ˆí™•ì‹¤ì„±, ë°ì´í„° ì¡ìŒ, ë˜ëŠ” ë¹„ì„ í˜• êµ¬ì¡°ì— ì˜í•´ ë°œìƒí–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆê¹Œ?
            - ì¼ê´€ì„±ì´ ë†’ì€ ë³€ìˆ˜êµ°ì´ ëª¨ë¸ì´ ì‹ ë¢°í•  ë§Œí•œ êµ¬ì¡°ì  ìš”ì¸ì„ ë°˜ì˜í•˜ê³  ìˆëŠ”ì§€ ë…¼ì˜í•˜ì‹­ì‹œì˜¤.
        
        (3) **Sensitivity (ë¯¼ê°ë„) ë¶„ì„:**
            - ì…ë ¥ê°’ì˜ ì‘ì€ ë³€í™”ì— í° ì˜ˆì¸¡ ë³€í™”ê°€ ë°œìƒí•œ ë³€ìˆ˜ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?
            - ë¯¼ê°ë„ê°€ ë†’ë‹¤ëŠ” ê²ƒì€ í•´ë‹¹ featureê°€ ë‹¨ê¸° ì‹œì¥ ë³€ë™ì„± ë˜ëŠ” ê³¼ë¯¼ ë°˜ì‘ì— ë¯¼ê°í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 
              ì´ëŸ¬í•œ featureë“¤ì´ í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ìŠ¤í¬ë‚˜ ë‹¨ê¸° íŠ¸ë ˆì´ë”© ì „ëµì— ì–´ë–¤ ì‹œì‚¬ì ì„ ì£¼ëŠ”ì§€ ë¶„ì„í•˜ì‹­ì‹œì˜¤.
            - ë¯¼ê°ë„ê°€ ë‚®ì€ featureëŠ” ì–´ë–¤ ì•ˆì •ì  ìš”ì¸ì„ ë°˜ì˜í•˜ëŠ”ì§€ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.
        
        (4) **Stability (ì•ˆì •ì„±) ë¶„ì„:**
            - í•™ìŠµ êµ¬ê°„ì´ë‚˜ ìƒ˜í”Œë§ ë³€í™”ì— ë”°ë¼ feature ì¤‘ìš”ë„ì˜ ë³€ë™ í­ì´ í° ë³€ìˆ˜ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?
            - ë³€ë™ì„±ì´ ë†’ì€ ë³€ìˆ˜ëŠ” ì‹œì¥ êµ­ë©´ ì „í™˜ì´ë‚˜ ë‰´ìŠ¤ ì´ë²¤íŠ¸ì— ë°˜ì‘í•  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.
            - ë°˜ëŒ€ë¡œ ë³€ë™ì„±ì´ ë‚®ì€ ë³€ìˆ˜ë“¤ì€ êµ¬ì¡°ì Â·ì¥ê¸°ì  íŠ¸ë Œë“œì— ì—°ë™ëœ ìš”ì¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
              ì´ëŸ¬í•œ ì°¨ì´ë¥¼ ê¸ˆìœµì ìœ¼ë¡œ í•´ì„í•˜ì‹­ì‹œì˜¤.
        
        (5) **í†µí•© ê²°ë¡  (Integrated Insight):**
            - ìœ„ ë„¤ ê°€ì§€ ê´€ì ì„ ì¢…í•©í•˜ì—¬ ì´ë²ˆ ì˜ˆì¸¡ì˜ ì£¼ìš” ì›ë™ë ¥ì„ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.
            - ì–´ë–¤ ë³€ìˆ˜ ì¡°í•©ì´ í–¥í›„ ê°€ê²© ì›€ì§ì„ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ”ì§€ ë…¼ë¦¬ì ìœ¼ë¡œ ì œì‹œí•˜ì‹­ì‹œì˜¤.
            - ëª¨ë¸ì˜ ì‹ ë¢°ì„±ê³¼ í•´ì„ ê°€ëŠ¥ì„±ì„ ë™ì‹œì— ê³ ë ¤í•˜ì—¬, ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•œ ì „ë¬¸ê°€ì  í‰ê°€ë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

        
        ---
        ì¶”ê°€ ë§¥ë½:
        ìµœê·¼ ì¢…ê°€: {getattr(stock_data, 'last_price', 'N/A')}
        ì˜ˆì¸¡ ì¢…ê°€: {getattr(target, 'next_close', 'N/A')}
        """

        # 3ï¸âƒ£ ë©”ì‹œì§€ ë¹Œë“œ (system + user)
        msg_sys = self._msg("system", sys_text)
        msg_user = self._msg("user", user_text)

        # 4ï¸âƒ£ LLM í˜¸ì¶œ
        parsed = self._ask_with_fallback(msg_sys, msg_user, self.schema_obj_opinion)
        reason = parsed.get("reason") or "(ì‚¬ìœ  ìƒì„± ì‹¤íŒ¨: ë¯¸ì…ë ¥)"

        return reason



    #[base_agent.py]
    def _msg(self, role: str, content: str) -> dict:
        """OpenAI ChatCompletionìš© ë©”ì‹œì§€ êµ¬ì¡° ìƒì„±"""
        if not isinstance(role, str) or not isinstance(content, str):
            raise ValueError(f"_msg() ì¸ì ì˜¤ë¥˜: role={role}, content={type(content)}")
        return {"role": role, "content": content}


    #[base_agent.py] OpenAI API í˜¸ì¶œ
    def _ask_with_fallback(self, msg_sys: dict, msg_user: dict, schema_obj: dict) -> dict:
        """Chat Completions API í˜¸ì¶œ (fallback ì§€ì›)"""
        last_err = None
        for model in self.preferred_models:
            payload = {
                "model": model,
                "messages": [msg_sys, msg_user],
                "temperature": self.temperature,
                "response_format": {
                    "type": "json_schema",
                    "json_schema": {
                        "name": "Response",
                        "schema": schema_obj
                    }
                }
            }
            try:
                import requests
                r = requests.post(self.OPENAI_URL, headers=self.headers, json=payload, timeout=120)
                if r.ok:
                    data = r.json()
                    # ìµœì‹  Chat APIì˜ ì‘ë‹µ ì²˜ë¦¬
                    msg = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                    if not msg:
                        continue
                    try:
                        return json.loads(msg)
                    except Exception:
                        return {"reason": msg.strip()}
                else:
                    last_err = (r.status_code, r.text)
                    continue
            except Exception as e:
                last_err = str(e)
                continue
        raise RuntimeError(f"ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_err}")

    def _p(self, msg: str):
        if self.verbose:
            print(f"[{self.agent_id}] {msg}")






# ==============================================================
# GradientAnalyzer (Integrated Gradients (IG) ì™€ Gradient Ã— Input (GÃ—I))
# ==============================================================
class GradientAnalyzer:
    """
    Gradient Ã— Input + Integrated Gradients ê¸°ë°˜ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ê¸°
    - SHAPì„ ëŒ€ì²´í•˜ë©°, LSTM ë“± ì‹œê³„ì—´ ëª¨ë¸ì—ë„ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘
    - ë‘ ë°©ë²• ê°„ ìƒê´€ê³„ìˆ˜ë¥¼ í†µí•´ ì¼ê´€ì„± ê²€ì¦ ë° ì¤‘ìš”ë„ í†µí•©
    """

    def __init__(self, model, feature_names, baseline=None, steps:int=50):
        self.model = model
        self.feature_names = feature_names
        self.baseline = baseline
        self.steps = steps

    # ------------------------------------------------------------
    # 1ï¸âƒ£ Gradient Ã— Input ê³„ì‚°
    # ------------------------------------------------------------
    def compute_gradient_x_input(self, x_input: np.ndarray) -> np.ndarray:
        """
        Gradient Ã— Input ê³„ì‚° (PyTorch)
        - ì…ë ¥ ì°¨ì›ì„ (batch, time, features) í˜•íƒœë¡œ ê°•ì œ ì •ê·œí™”
        - (1, 1, 40, 169) ê°™ì€ ì˜ëª»ëœ ì…ë ¥ë„ ìë™ ìˆ˜ì •
        """
        # âœ… ì°¨ì› ì •ê·œí™”
        x_input = np.array(x_input, dtype=np.float32)
        if x_input.ndim == 4:
            # (1, 1, 40, features) -> (1, 40, features)
            x_input = np.squeeze(x_input, axis=1)
        elif x_input.ndim == 2:
            # (40, features) -> (1, 40, features)
            x_input = np.expand_dims(x_input, axis=0)

        # âœ… PyTorch Tensor ë³€í™˜ ë° Gradient ê³„ì‚°
        device = next(self.model.parameters()).device
        x = torch.FloatTensor(x_input).to(device)
        x.requires_grad_(True)
        
        self.model.eval()
        preds = self.model(x)
        
        # Gradient ê³„ì‚°
        grads = torch.autograd.grad(
            outputs=preds.sum(),
            inputs=x,
            create_graph=False,
            retain_graph=False
        )[0]
        
        gx = torch.abs(grads * x)

        return gx.detach().cpu().numpy()


    # ------------------------------------------------------------
    # 2ï¸âƒ£ Integrated Gradients ê³„ì‚°
    # ------------------------------------------------------------
    def compute_integrated_gradients(self, x_input: np.ndarray) -> np.ndarray:
        # âœ… ì°¨ì› ì •ë¦¬: (batch, time, features)
        x_input = np.array(x_input, dtype=np.float32)
        if x_input.ndim == 4:
            # (steps, 1, 40, features) or (1, 1, 40, features)
            x_input = np.squeeze(x_input, axis=1)
        if x_input.ndim == 2:
            # (40, features) -> (1, 40, features)
            x_input = np.expand_dims(x_input, axis=0)

        if self.baseline is None:
            self.baseline = np.zeros_like(x_input)

        # âœ… baselineê³¼ shape ë™ì¼ í™•ì¸
        assert self.baseline.shape == x_input.shape, \
            f"Baseline shape {self.baseline.shape} != x_input {x_input.shape}"

        interpolated = [
            self.baseline + (float(i)/self.steps)*(x_input - self.baseline)
            for i in range(self.steps + 1)
        ]
        interpolated = np.array(interpolated, dtype=np.float32)  # (steps+1, 1, 40, features) í˜•íƒœ
        interpolated = np.squeeze(interpolated, axis=1)          # âœ… (steps+1, 40, features)

        # âœ… PyTorch Tensor ë³€í™˜ ë° Gradient ê³„ì‚°
        device = next(self.model.parameters()).device
        interpolated_torch = torch.FloatTensor(interpolated).to(device)
        interpolated_torch.requires_grad_(True)
        
        self.model.eval()
        preds = self.model(interpolated_torch)

        # Gradient ê³„ì‚°
        grads = torch.autograd.grad(
            outputs=preds.sum(),
            inputs=interpolated_torch,
            create_graph=False,
            retain_graph=False
        )[0]
        
        avg_grads = torch.mean(grads[:-1], dim=0)
        ig = (x_input - self.baseline) * avg_grads.detach().cpu().numpy()

        return ig

    # ------------------------------------------------------------
    # 3ï¸âƒ£ ë³‘í•© ì‹¤í–‰ (SHAP ëŒ€ì²´)
    # ------------------------------------------------------------
    def run_all_gradients(self, x_input: np.ndarray):
        """
        Gradient Ã— Input + Integrated Gradientsë¥¼ ë™ì‹œì— ìˆ˜í–‰í•˜ê³ 
        6ê°€ì§€ summary êµ¬ì¡°ë¡œ feature importanceë¥¼ ë°˜í™˜í•˜ëŠ” ë²„ì „.
        """

        # Gradient analysis ì‹¤í–‰ ì¤‘...

        # 1ï¸âƒ£ Gradient Ã— Input / Integrated Gradients ê³„ì‚°
        gx = self.compute_gradient_x_input(x_input)
        ig = self.compute_integrated_gradients(x_input)

        gx_mean = np.mean(np.abs(gx), axis=(0, 1))
        ig_mean = np.mean(np.abs(ig), axis=(0, 1))

        feature_names = np.array(self.feature_names)
        importance_df = pd.DataFrame({
            "feature": feature_names,
            "gradxinput": gx_mean,
            "integrated_gradients": ig_mean
        })

        # 2ï¸âƒ£ ë‘ attributionì˜ í‰ê· ì„ ìµœì¢… ì¤‘ìš”ë„ë¡œ ì‚¬ìš©
        importance_df["final_importance"] = (
                0.5 * (importance_df["gradxinput"] + importance_df["integrated_gradients"])
        )

        # 3ï¸âƒ£ ì¼ê´€ì„±(agreement ratio)
        corr = np.corrcoef(gx_mean, ig_mean)[0, 1]
        # IGâ€“GÃ—I correlation: {corr:.4f}

        # 4ï¸âƒ£ feature summary (í•µì‹¬ ìš”ì•½)
        feature_summary = {
            "agreement_ratio": float(corr),
            "gx_importance_top": importance_df.sort_values("gradxinput", ascending=False).head(3)["feature"].tolist(),
            "ig_importance_top": importance_df.sort_values("integrated_gradients", ascending=False).head(3)["feature"].tolist()
        }

        # 5ï¸âƒ£ importance dict
        importance_dict = dict(
            zip(feature_names, importance_df["final_importance"])
        )

        # 6ï¸âƒ£ temporal summary (ìƒìœ„ 5ê°œ feature ì„¸ë¶€ìš”ì•½)
        temporal_summary = (
            importance_df.sort_values("final_importance", ascending=False)
            .head(5)
            .to_dict(orient="records")
        )

        # 7ï¸âƒ£ consistency summary (IG vs GÃ—I ìˆœìœ„ ì¼ì¹˜ë„)
        ig_rank = importance_df.sort_values("integrated_gradients", ascending=False).reset_index(drop=True)
        gx_rank = importance_df.sort_values("gradxinput", ascending=False).reset_index(drop=True)
        consistency_summary = []
        for f in feature_names:
            ig_pos = ig_rank[ig_rank["feature"] == f].index[0]
            gx_pos = gx_rank[gx_rank["feature"] == f].index[0]
            rank_gap = abs(int(ig_pos) - int(gx_pos))
            if rank_gap > 10:  # ìˆœìœ„ ì°¨ì´ê°€ í° featureë§Œ ì €ì¥
                consistency_summary.append({"feature": f, "rank_gap": rank_gap})

        # 8ï¸âƒ£ sensitivity summary (gradient í‘œì¤€í¸ì°¨ ê¸°ë°˜ ë¯¼ê°ë„)
        grads = np.abs(gx)
        sensitivity_summary = [
            {"feature": f, "sensitivity": float(np.std(grads[:, :, i]))}
            for i, f in enumerate(feature_names)
        ]
        sensitivity_summary = sorted(sensitivity_summary, key=lambda x: x["sensitivity"], reverse=True)[:5]

        # 9ï¸âƒ£ stability summary (feature ì¤‘ìš”ë„ì˜ ë³€ë™ì„±)
        importance_df["variance"] = importance_df[["gradxinput", "integrated_gradients"]].var(axis=1)
        stability_summary = (
            importance_df.sort_values("variance", ascending=False)
            .head(5)
            .to_dict(orient="records")
        )


        # ğŸ”Ÿ ëª¨ë“  summary í†µí•©
        grad_results = {
            "feature_summary": feature_summary,
            "importance_dict": importance_dict,
            "temporal_summary": temporal_summary,
            "consistency_summary": consistency_summary,
            "sensitivity_summary": sensitivity_summary,
            "stability_summary": stability_summary
        }

        # Gradient analysis ì™„ë£Œ
        return (importance_dict, pd.DataFrame(temporal_summary), pd.DataFrame(consistency_summary),
                pd.DataFrame(sensitivity_summary), grad_results)
